{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pickle as cPickle\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import python_speech_features as mfcc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "source   = \"development_set\\\\\"\n",
    "dest = \"speaker_models\\\\\"\n",
    "train_file = \"development_set_enroll_3.txt\" # \"train.txt\" # \n",
    "n_files = 3 #training_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import python_speech_features as mfcc\n",
    "\n",
    "def calculate_delta(array):\n",
    "    \"\"\"Calculate and returns the delta of given feature vector matrix\"\"\"\n",
    "\n",
    "    rows,cols = array.shape\n",
    "    deltas = np.zeros((rows,20))\n",
    "    N = 2\n",
    "    for i in range(rows):\n",
    "        index = []\n",
    "        j = 1\n",
    "        while j <= N:\n",
    "            if i-j < 0:\n",
    "              first =0\n",
    "            else:\n",
    "              first = i-j\n",
    "            if i+j > rows-1:\n",
    "                second = rows-1\n",
    "            else:\n",
    "                second = i+j \n",
    "            index.append((second,first))\n",
    "            j+=1\n",
    "        deltas[i] = ( array[index[0][0]]-array[index[0][1]] + (2 * (array[index[1][0]]-array[index[1][1]])) ) / 10\n",
    "    return deltas\n",
    "\n",
    "def extract_features(audio,rate):\n",
    "    \"\"\"on extrait 20 caracteristique de l'audio, et on calcule la variation pour augmenter le nombre de \n",
    "    caracteristique a 40\"\"\"    \n",
    "    \n",
    "    mfcc_feature = mfcc.mfcc(audio,rate, 0.045, 0.01,20,nfft = 2300, appendEnergy = True)    \n",
    "    mfcc_feature = preprocessing.scale(mfcc_feature)\n",
    "    delta = calculate_delta(mfcc_feature)\n",
    "    combined = np.hstack((mfcc_feature,delta)) \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sekou_1\\audio1.wav\n",
      "sekou_1\\audio2.wav\n",
      "sekou_1\\audio3.wav\n",
      "+ modeling completed for speaker: sekou_1.gmm  with data point =  (1115, 40)\n",
      "sekou_2\\audio1.wav\n",
      "sekou_2\\audio2.wav\n",
      "sekou_2\\audio3.wav\n",
      "+ modeling completed for speaker: sekou_2.gmm  with data point =  (1157, 40)\n",
      "hamadoun_1\\audio1.wav\n",
      "hamadoun_1\\audio2.wav\n",
      "hamadoun_1\\audio3.wav\n",
      "+ modeling completed for speaker: hamadoun_1.gmm  with data point =  (1188, 40)\n",
      "hamadoun_2\\audio1.wav\n",
      "hamadoun_2\\audio2.wav\n",
      "hamadoun_2\\audio3.wav\n",
      "+ modeling completed for speaker: hamadoun_2.gmm  with data point =  (1188, 40)\n",
      "djibson_1\\audio1.wav\n",
      "djibson_1\\audio2.wav\n",
      "djibson_1\\audio3.wav\n",
      "+ modeling completed for speaker: djibson_1.gmm  with data point =  (1069, 40)\n",
      "djibson_2\\audio1.wav\n",
      "djibson_2\\audio2.wav\n",
      "djibson_2\\audio3.wav\n",
      "+ modeling completed for speaker: djibson_2.gmm  with data point =  (1037, 40)\n"
     ]
    }
   ],
   "source": [
    "# Extracting features for each digit data provided by speaker \n",
    "file_paths = open(train_file,'r')\n",
    "count = 1\n",
    "features = np.asarray(())\n",
    "for path in file_paths:    \n",
    "    path = path.strip()   \n",
    "    print(path)\n",
    "    \n",
    "    # read the audio\n",
    "    sr,audio = read(source + path)\n",
    "    \n",
    "    # extract 40 dimensional MFCC & delta MFCC features\n",
    "    vector   = extract_features(audio,sr)\n",
    "    \n",
    "    if features.size == 0:\n",
    "        features = vector\n",
    "    else:\n",
    "        features = np.vstack((features, vector))\n",
    "    # when features of training files of each digit for a given speaker are concatenated, model training is done\n",
    "    if count == n_files:    \n",
    "        gmm = GMM(n_components = 16, max_iter = 200, covariance_type='diag',n_init = 3)\n",
    "        gmm.fit(features)\n",
    "        \n",
    "        # dumping the trained gaussian model\n",
    "        picklefile = path.split(\"\\\\\")[0]+\".gmm\"\n",
    "        pickle.dump(gmm,open(dest + picklefile,'wb'))\n",
    "        print('+ modeling completed for speaker:',picklefile,\" with data point = \",features.shape)    \n",
    "        features = np.asarray(())\n",
    "        count = 0\n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['speaker_models\\\\djibson_1.gmm',\n",
       " 'speaker_models\\\\djibson_2.gmm',\n",
       " 'speaker_models\\\\hamadoun_1.gmm',\n",
       " 'speaker_models\\\\hamadoun_2.gmm',\n",
       " 'speaker_models\\\\hassan_1.gmm',\n",
       " 'speaker_models\\\\hassan_2.gmm',\n",
       " 'speaker_models\\\\ibh_1.gmm',\n",
       " 'speaker_models\\\\ibh_2.gmm',\n",
       " 'speaker_models\\\\makaveli_1.gmm',\n",
       " 'speaker_models\\\\namory_1.gmm',\n",
       " 'speaker_models\\\\namory_2.gmm',\n",
       " 'speaker_models\\\\sekou_1.gmm',\n",
       " 'speaker_models\\\\sekou_2.gmm']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time, os\n",
    "\n",
    "test_file = \"development_set_test.txt\"  # \"test.txt\" #     \n",
    "\n",
    "file_paths = open(test_file,'r')\n",
    "\n",
    "gmm_files = [os.path.join(dest,fname) for fname in os.listdir(dest) if fname.endswith('.gmm')]\n",
    "\n",
    "gmm_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3),\n",
       " GaussianMixture(covariance_type='diag', max_iter=200, n_components=16, n_init=3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the Gaussian Models\n",
    "models    = [pickle.load(open(fname,'rb')) for fname in gmm_files]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['djibson_1',\n",
       " 'djibson_2',\n",
       " 'hamadoun_1',\n",
       " 'hamadoun_2',\n",
       " 'hassan_1',\n",
       " 'hassan_2',\n",
       " 'ibh_1',\n",
       " 'ibh_2',\n",
       " 'makaveli_1',\n",
       " 'namory_1',\n",
       " 'namory_2',\n",
       " 'sekou_1',\n",
       " 'sekou_2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speakers_digit   = [fname.split(\"\\\\\")[-1].split(\".gmm\")[0] for fname in gmm_files]\n",
    "speakers_digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sekou_1\\audio4.wav\n",
      "\tSpeaker & Digit detected -  sekou_1\n",
      "sekou_2\\audio4.wav\n",
      "\tSpeaker & Digit detected -  sekou_2\n",
      "hamadoun_1\\audio4.wav\n",
      "\tSpeaker & Digit detected -  hamadoun_1\n",
      "hamadoun_1\\audio5.wav\n",
      "\tSpeaker & Digit detected -  hamadoun_1\n",
      "hamadoun_1\\audio6.wav\n",
      "\tSpeaker & Digit detected -  hamadoun_1\n",
      "hamadoun_2\\audio4.wav\n",
      "\tSpeaker & Digit detected -  hamadoun_2\n",
      "djibson_1\\audio4.wav\n",
      "\tSpeaker & Digit detected -  djibson_1\n",
      "djibson_2\\audio4.wav\n",
      "\tSpeaker & Digit detected -  ibh_2\n",
      "test_1\\audio4.wav\n",
      "\tSpeaker & Digit detected -  ibh_1\n",
      "test_2\\audio4.wav\n",
      "\tSpeaker & Digit detected -  ibh_2\n",
      "ibh_1\\audio4.wav\n",
      "\tSpeaker & Digit detected -  ibh_1\n",
      "ibh_2\\audio4.wav\n",
      "\tSpeaker & Digit detected -  ibh_2\n"
     ]
    }
   ],
   "source": [
    "# Read the test directory and get the list of test audio files \n",
    "path_predicted_dict = dict()\n",
    "for path in file_paths:   \n",
    "    \n",
    "    path = path.strip()   \n",
    "    #print(path)\n",
    "    sr,audio = read(source + path)\n",
    "    vector   = extract_features(audio,sr)\n",
    "    \n",
    "    log_likelihood = np.zeros(len(models)) \n",
    "    \n",
    "    for i in range(len(models)):\n",
    "        gmm    = models[i]  #checking with each model one by one\n",
    "        cluster=gmm.predict(vector)\n",
    "        \n",
    "        scores = np.array(gmm.score(vector))\n",
    "        log_likelihood[i] = scores.sum()\n",
    "\n",
    "    \n",
    "    winner = np.argmax(log_likelihood)\n",
    "    #print(log_likelihood)\n",
    "    print(path)\n",
    "    print(\"\\tSpeaker & Digit detected - \", speakers_digit[winner])\n",
    "    path_predicted_dict[path] = speakers_digit[winner]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Dataset =  75.0 %\n"
     ]
    }
   ],
   "source": [
    "correct_pred = 0\n",
    "incorrect_pred = 0\n",
    "for i in path_predicted_dict.keys():\n",
    "    if i.split(\"\\\\\")[0] == path_predicted_dict[i]:\n",
    "        correct_pred += 1\n",
    "    else:\n",
    "        incorrect_pred += 1\n",
    "\n",
    "print(\"Accuracy on Test Dataset = \", correct_pred*100/(correct_pred + incorrect_pred), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from scipy.io.wavfile import read\n",
    "from IPython.display import Audio, display, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dites votre commande dans 3 seconds\n",
      "Dites votre commande dans 2 seconds\n",
      "Dites votre commande dans 1 seconds\n",
      "Dites votre commande dans 0 seconds\n",
      "recording...\n",
      "Done\n",
      "Predicted as: sekou\n",
      "ouverture de la porte\n"
     ]
    }
   ],
   "source": [
    " #Voice authentication\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "CHUNK = 1024\n",
    "RECORD_SECONDS = 2\n",
    "\n",
    "audio = pyaudio.PyAudio()\n",
    "\n",
    "j=3\n",
    "while j>=0:\n",
    "    time.sleep(1.0)\n",
    "    os.system('cls' if os.name == 'nt' else 'clear')\n",
    "    print(\"Dites votre commande dans {} seconds\".format(j))\n",
    "    j-=1\n",
    "\n",
    "# start Recording\n",
    "stream = audio.open(format=FORMAT, channels=CHANNELS,\n",
    "rate=RATE, input=True,\n",
    "frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"recording...\")\n",
    "frames = []\n",
    "\n",
    "for _ in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "# stop Recording\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "audio.terminate()\n",
    "        \n",
    "# saving wav file of speaker\n",
    "waveFile = wave.open('test.wav', 'wb')\n",
    "waveFile.setnchannels(CHANNELS)\n",
    "waveFile.setsampwidth(audio.get_sample_size(FORMAT))\n",
    "waveFile.setframerate(RATE)\n",
    "waveFile.writeframes(b''.join(frames))\n",
    "waveFile.close()\n",
    "print(\"Done\")\n",
    "Fs, audio = read(\"test.wav\")\n",
    "vector =  extract_features(audio,Fs)\n",
    "log_likelihood = np.zeros(len(models)) \n",
    "    \n",
    "for i in range(len(models)):\n",
    "    gmm    = models[i]  #checking with each model one by one\n",
    "    cluster=gmm.predict(vector)\n",
    "        \n",
    "    scores = np.array(gmm.score(vector))\n",
    "    log_likelihood[i] = scores.sum()\n",
    "    \n",
    "    \n",
    "winner = np.argmax(log_likelihood)\n",
    "\n",
    "result=speakers_digit[winner].split(\"_\")\n",
    "print(\"Predicted as:\",result[0] )\n",
    "if(result[1]==\"1\"):\n",
    "    print(\"ouverture de la porte\")\n",
    "else:\n",
    "    print(\"validation de la seance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Test-2.wav'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-9cebab48ef63>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mchannels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwave\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mspf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0msampleRate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetframerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mampWidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetsampwidth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\wave.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(f, mode)\u001b[0m\n\u001b[0;32m    508\u001b[0m             \u001b[0mmode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 510\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mWave_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    511\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mWave_write\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\anaconda3\\lib\\wave.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_i_opened_the_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 160\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    161\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_i_opened_the_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[1;31m# else, assume it is an open file object already\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Test-2.wav'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import contextlib\n",
    "\n",
    "fname = 'Test-2.wav'\n",
    "outname = 'filtered.wav'\n",
    "\n",
    "cutOffFrequency = 400.0\n",
    "\n",
    "# \n",
    "def running_mean(x, windowSize):\n",
    "  cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "  return (cumsum[windowSize:] - cumsum[:-windowSize]) / windowSize\n",
    "\n",
    "# from http://stackov\n",
    "def interpret_wav(raw_bytes, n_frames, n_channels, sample_width, interleaved = True):\n",
    "\n",
    "    if sample_width == 1:\n",
    "        dtype = np.uint8 # unsigned char\n",
    "    elif sample_width == 2:\n",
    "        dtype = np.int16 # signed 2-byte short\n",
    "    else:\n",
    "        raise ValueError(\"Only supports 8 and 16 bit audio formats.\")\n",
    "\n",
    "    channels = np.fromstring(raw_bytes, dtype=dtype)\n",
    "\n",
    "    if interleaved:\n",
    "        # channels are interleaved, i.e. sample N of channel M follows sample N of channel M-1 in raw data\n",
    "        channels.shape = (n_frames, n_channels)\n",
    "        channels = channels.T\n",
    "    else:\n",
    "        # channels are not interleaved. All samples from channel M occur before all samples from channel M-1\n",
    "        channels.shape = (n_channels, n_frames)\n",
    "\n",
    "    return channels\n",
    "\n",
    "with contextlib.closing(wave.open(fname,'rb')) as spf:\n",
    "    sampleRate = spf.getframerate()\n",
    "    ampWidth = spf.getsampwidth()\n",
    "    nChannels = spf.getnchannels()\n",
    "    nFrames = spf.getnframes()\n",
    "\n",
    "    # Extract Raw Audio from multi-channel Wav File\n",
    "    signal = spf.readframes(nFrames*nChannels)\n",
    "    spf.close()\n",
    "    channels = interpret_wav(signal, nFrames, nChannels, ampWidth, True)\n",
    "\n",
    "    # get window size\n",
    "    # from http://dsp.stackexchange.com/questions/9966/what-is-the-cut-off-frequency-of-a-moving-average-filter\n",
    "    freqRatio = (cutOffFrequency/sampleRate)\n",
    "    N = int(math.sqrt(0.196196 + freqRatio**2)/freqRatio)\n",
    "\n",
    "    # Use moviung average (only on first channel)\n",
    "    filtered = running_mean(channels[0], N).astype(channels.dtype)\n",
    "\n",
    "    wav_file = wave.open(outname, \"w\")\n",
    "    wav_file.setparams((1, ampWidth, sampleRate, nFrames, spf.getcomptype(), spf.getcompname()))\n",
    "    wav_file.writeframes(filtered.tobytes('C'))\n",
    "    wav_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
